{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "289ebffa-0fef-45c0-a9dc-3ce52103cc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b3589f9-7eaf-455b-8e85-5a59479bdf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Read the dataset:\n",
    "file = open(\"/Users/diego/Scripts/og-language-models/tiny-cicero.txt\", \"r\")\n",
    "contents = file.read()\n",
    "#print(contents)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d067c9f-44ed-4e55-ba97-4d1cc7772cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', '!', '$', '&', '(', ')', ',', '-', '.', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '£', 'Æ', 'ä', 'æ', 'è', 'é', 'Œ', 'œ', 'α', 'δ', 'μ', 'ν', 'ο', 'π', 'ς', 'τ', '—', '‘', '’', '“', '”', '\\ufeff']\n"
     ]
    }
   ],
   "source": [
    "vocabulary = list(set(contents))\n",
    "vocabulary = sorted(vocabulary)\n",
    "VOCAB_SIZE = len(vocabulary)\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4896c6a-1e1f-4a9b-9cc6-bf215e10ee74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Length:  86\n",
      "Content Length:  851509\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary Length: \", len(vocabulary))\n",
    "print(\"Content Length: \", len(contents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5761a703-34cd-44f4-ad32-7cff767b1ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19, 42, 49, 49, 52, 0, 34, 52, 55, 49, 41]\n",
      "Hello World\n"
     ]
    }
   ],
   "source": [
    "## First we must make encode and decoder functions for our dataset\n",
    "string_to_int = {ch : i for i, ch in enumerate(vocabulary)}\n",
    "int_to_string = {i : ch for i, ch in enumerate(vocabulary)}\n",
    "encode = lambda s : [string_to_int[c] for c in s]\n",
    "decode = lambda l : ''.join([int_to_string[i] for i in l])\n",
    "print(encode(\"Hello World\"))\n",
    "print(decode(encode(\"Hello World\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f11b58ba-50c2-4fc2-88b7-76ce0af285b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([851509]) torch.int64\n",
      "tensor([85, 26, 25,  0, 17, 29, 20, 16, 25, 15, 30, 19, 20, 27,  0, 24, 12, 29,\n",
      "        14, 32, 30,  0, 31, 32, 23, 23, 20, 32, 30,  0, 14, 20, 14, 16, 29, 26,\n",
      "         0, 31, 19, 16,  0, 38, 58, 44, 58, 55,  0, 28, 58, 46, 51, 57, 58, 56,\n",
      "         0, 24, 58, 40, 46, 58, 56,  0, 30, 40, 38, 42, 59, 52, 49, 38,  0, 58,\n",
      "        56, 42, 41,  0, 57, 52,  0, 55, 42, 40, 52, 58, 51, 57,  0, 38,  0, 51,\n",
      "        58, 50, 39, 42, 55,  0, 52, 43,  0, 56, 57, 52, 55, 46, 42, 56,  0, 38,\n",
      "        39, 52, 58, 57,  0, 45, 46, 56,  0, 43, 38, 57, 45, 42, 55,  7, 46, 51,\n",
      "         7, 49, 38, 60,  0, 18, 38, 46, 58, 56,  0, 23, 38, 42, 49, 46, 58, 56,\n",
      "         6,  0, 38, 40, 40, 58, 55, 38, 57, 42, 49, 62,  0, 55, 42, 50, 42, 50,\n",
      "        39, 42, 55, 42, 41,  0, 38, 51, 41,  0, 40, 45, 38, 55, 50, 46, 51, 44,\n",
      "        49, 62,  0, 57, 52, 49, 41, 10,  0, 38, 51, 41,  0, 60, 45, 42, 51, 42,\n",
      "        59, 42, 55,  0, 45, 42,  0, 57, 38, 49, 48, 42, 41,  0, 38, 39, 52, 58,\n",
      "        57,  0, 45, 46, 50,  0, 38, 49, 60, 38, 62, 56,  0, 44, 38, 59, 42,  0,\n",
      "        45, 46, 50,  0, 57, 45, 42,  0, 57, 46, 57, 49, 42,  0, 52, 43,  0, 83,\n",
      "        57, 45, 42,  0, 60, 46, 56, 42, 84,  0, 60, 46, 57, 45, 52, 58, 57,  0,\n",
      "        38, 51, 62,  0, 45, 42, 56, 46, 57, 38, 57, 46, 52, 51,  8,  0, 20,  0,\n",
      "        45, 38, 41,  0, 39, 42, 42, 51,  0, 46, 51, 57, 55, 52, 41, 58, 40, 42,\n",
      "        41,  0, 39, 62,  0, 50, 62,  0, 43, 38, 57, 45, 42, 55,  0, 57, 52,  0,\n",
      "        30, 40, 38, 42, 59, 52, 49, 38,  0, 38, 56,  0, 56, 52, 52, 51,  0, 38,\n",
      "        56,  0, 20,  0, 45, 38, 41,  0, 38, 56, 56, 58, 50, 42, 41,  0, 57, 45,\n",
      "        42,  0, 57, 52, 44, 38,  0, 59, 46, 55, 46, 49, 46, 56,  6,  0, 38, 51,\n",
      "        41,  0, 20,  0, 57, 52, 52, 48,  0, 38, 41, 59, 38, 51, 57, 38, 44, 42,\n",
      "         0, 52, 43,  0, 57, 45, 42,  0, 46, 51, 57, 55, 52, 41, 58, 40, 57, 46,\n",
      "        52, 51,  0, 51, 42, 59, 42, 55,  0, 57, 52,  0, 54, 58, 46, 57,  0, 57,\n",
      "        45, 42,  0, 59, 42, 51, 42, 55, 38, 39, 49, 42,  0, 50, 38, 51, 82, 56,\n",
      "         0, 56, 46, 41, 42,  0, 38, 56,  0, 49, 52, 51, 44,  0, 38, 56,  0, 20,\n",
      "         0, 60, 38, 56,  0, 38, 39, 49, 42,  0, 57, 52,  0, 56, 57, 38, 62,  0,\n",
      "        38, 51, 41,  0, 45, 42,  0, 60, 38, 56,  0, 56, 53, 38, 55, 42, 41,  0,\n",
      "        57, 52,  0, 58, 56,  8,  0, 31, 45, 42,  0, 40, 52, 51, 56, 42, 54, 58,\n",
      "        42, 51, 40, 42,  0, 60, 38, 56,  0, 57, 45, 38, 57,  0, 20,  0, 40, 52,\n",
      "        50, 50, 46, 57, 57, 42, 41,  0, 57, 52,  0, 50, 42, 50, 52, 55, 62,  0,\n",
      "        50, 38, 51, 62,  0, 41, 46, 56, 54, 58, 46, 56, 46, 57, 46, 52, 51, 56,\n",
      "         0, 52, 43,  0, 45, 46, 56,  6,  0, 38, 56,  0, 60, 42, 49, 49,  0, 38,\n",
      "        56,  0, 50, 38, 51, 62,  0, 56, 45, 52, 55, 57,  0, 53, 52, 46, 51, 57,\n",
      "        42, 41,  0, 38, 53, 52, 53, 45, 57, 45, 42, 44, 50, 56,  6,  0, 38, 51,\n",
      "        41,  6,  0, 46, 51,  0, 56, 45, 52, 55, 57,  6,  0, 57, 52, 52, 48,  0,\n",
      "        38, 56,  0, 50, 58, 40, 45,  0, 38, 41, 59, 38, 51, 57, 38, 44, 42,  0,\n",
      "        52, 43,  0, 45, 46, 56,  0, 60, 46, 56, 41, 52, 50,  0, 38, 56,  0, 20,\n",
      "         0, 40, 52, 58, 49, 41,  8,  0, 34, 45, 42, 51,  0, 45, 42,  0, 41, 46,\n",
      "        42, 41,  6,  0, 20,  0, 38, 57, 57, 38, 40, 45, 42, 41,  0, 50, 62, 56,\n",
      "        42, 49, 43,  0, 57, 52,  0, 30, 40, 38, 42, 59, 52, 49, 38,  0, 57, 45,\n",
      "        42,  0, 27, 52, 51, 57, 46, 43, 42, 61,  6,  0, 60, 45, 52, 50,  0, 20,\n",
      "         0, 50, 38, 62,  0, 59, 42, 51, 57, 58, 55, 42,  0, 57, 52,  0, 40, 38,\n",
      "        49, 49,  0, 54, 58, 46, 57, 42,  0, 57, 45, 42,  0, 50, 52, 56, 57,  0,\n",
      "        41, 46, 56, 57, 46, 51, 44, 58, 46, 56, 45, 42, 41,  0, 52, 43,  0, 52,\n",
      "        58, 55,  0, 40, 52, 58, 51, 57, 55, 62, 50, 42, 51,  0, 43, 52, 55,  0,\n",
      "        38, 39, 46, 49, 46, 57, 62,  0, 38, 51, 41,  0, 58, 53, 55, 46, 44, 45,\n",
      "        57, 51, 42, 56, 56,  8,  0, 13, 58, 57,  0, 52, 43,  0, 57, 45, 46, 56,\n",
      "         0, 49, 38, 57, 57, 42, 55,  0, 20,  0, 56, 45, 38, 49, 49,  0, 57, 38,\n",
      "        48, 42,  0, 52, 57, 45, 42, 55,  0, 52, 40, 40, 38, 56, 46, 52, 51, 56,\n",
      "         0, 57, 52,  0, 56, 53, 42, 38, 48,  8,  0, 31, 52,  0, 55, 42, 57, 58,\n",
      "        55, 51,  0, 57, 52,  0, 30, 40, 38, 42, 59, 52, 49, 38,  0, 57, 45, 42,\n",
      "         0, 38, 58, 44, 58, 55,  8,  0, 12, 50, 52, 51, 44,  0, 50, 38, 51, 62,\n",
      "         0, 52, 57, 45, 42, 55,  0, 52, 40, 40, 38, 56, 46, 52, 51, 56,  0, 20,\n",
      "         0, 53, 38, 55, 57, 46, 40, 58, 49, 38, 55, 49, 62,  0, 55, 42, 50, 42,\n",
      "        50, 39, 42, 55,  0, 52, 51, 42,  8,  0])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(contents), dtype=int)\n",
    "print(data.shape,data.dtype)\n",
    "print(data[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae8d3705-c112-499e-b9bc-1491a914609f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9*len(data))\n",
    "train_data = data[n:]\n",
    "val_data = data[:n]\n",
    "train_data = train_data.float()\n",
    "val_data = val_data.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acf82293-5890-4813-a8c4-c1f9961d2ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now, we define our context window\n",
    "torch.manual_seed(1337)\n",
    "BATCH_SIZE = 4\n",
    "CONTEXT_SIZE = 8\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - CONTEXT_SIZE, (BATCH_SIZE,))\n",
    "    x = torch.stack([data[i:i+CONTEXT_SIZE] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+CONTEXT_SIZE+1] for i in ix])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46d0e620-c421-41fc-8c2c-a09bbf88329f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "tensor([[57., 46., 51., 44.,  0., 56., 52., 50.],\n",
      "        [42., 56.,  6.,  0., 30., 46., 55.,  6.],\n",
      "        [56., 42.,  0., 46., 51.,  0., 57., 45.],\n",
      "        [46., 56., 57., 42., 41.,  0., 38., 43.]])\n",
      "targets:\n",
      "tensor([[46., 51., 44.,  0., 56., 52., 50., 42.],\n",
      "        [56.,  6.,  0., 30., 46., 55.,  6.,  0.],\n",
      "        [42.,  0., 46., 51.,  0., 57., 45., 42.],\n",
      "        [56., 57., 42., 41.,  0., 38., 43., 57.]])\n"
     ]
    }
   ],
   "source": [
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "160dd1b6-8242-4e4d-8710-eabed0a835fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LSTM Model\n",
    "\n",
    "## Short Term Memory Block\n",
    "class stmBlock(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Forget Gate:\n",
    "        self.Wif = nn.Linear(input_dim, hidden_dim, bias = False)\n",
    "        self.Whf = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        # Input Gate:\n",
    "        self.Wii = nn.Linear(input_dim, hidden_dim, bias = False)\n",
    "        self.Whi = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        # Candidate Gate:\n",
    "        self.Wic = nn.Linear(input_dim, hidden_dim, bias = False)\n",
    "        self.Whc = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        # Output Gate:\n",
    "        self.Wio = nn.Linear(input_dim, hidden_dim, bias = False)\n",
    "        self.Who = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "\n",
    "    def forward(self, x, c_prev = None, h_prev = None):\n",
    "        # If not first block:\n",
    "        if h_prev is not None:\n",
    "            f = self.sigmoid(self.Wif(x) + self.Whf(h_prev))\n",
    "            i = self.sigmoid(self.Wii(x) + self.Whi(h_prev))\n",
    "            c = self.tanh(self.Wic(x) + self.Whc(h_prev))\n",
    "            o = self.sigmoid(self.Wio(x) + self.Who(h_prev))\n",
    "        else:\n",
    "            f = self.sigmoid(self.Wif(x))\n",
    "            i = self.sigmoid(self.Wii(x))\n",
    "            c = self.tanh(self.Wic(x))\n",
    "            o = self.sigmoid(self.Wio(x))\n",
    "\n",
    "        if c_prev == None:\n",
    "            c_t = i * c\n",
    "        else:\n",
    "            c_t = f * c_prev + i * c\n",
    "        h = o * self.tanh(c_t)\n",
    "        return c_t, h\n",
    "\n",
    "## Now for the entire LSTM Module\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.stmBlocks = nn.ModuleList([stmBlock(input_dim, hidden_dim) for i in range(CONTEXT_SIZE)])\n",
    "        # We make an additional list of linear layers for the ouput of each block\n",
    "        self.linLays = nn.ModuleList([nn.Linear(hidden_dim, output_dim) for i in range(CONTEXT_SIZE)])\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        logits = []\n",
    "        h = None\n",
    "        c = None\n",
    "        for i in range(CONTEXT_SIZE):\n",
    "            element = x[:, i].unsqueeze(1)\n",
    "            print(\"Element \", element)\n",
    "            c, h = self.stmBlocks[i](c, h, element) if h is not None else self.stmBlocks[i](element)\n",
    "            print(\"h \", h.shape)\n",
    "            o = self.linLays[i](h)\n",
    "            print(\"Ouput: \", o.shape)\n",
    "            logits.append(o)\n",
    "        \n",
    "        print(len(logits))\n",
    "        logits = torch.stack(logits, dim=1)\n",
    "        print(logits.shape)\n",
    "        if targets is not None:\n",
    "            loss = self.loss(logits.view(-1, VOCAB_SIZE), targets.view(-1).long())\n",
    "            return logits, loss\n",
    "        return logits, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "2491b4b0-e433-401f-accf-30e6badf785d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model):\n",
    "    lossAvg = 0\n",
    "    counter = 0\n",
    "    for i in range(400):\n",
    "        counter+=1\n",
    "        batch = get_batch(\"test\")\n",
    "        logits, loss = model(batch[0], batch[1])\n",
    "        lossAvg += loss.item()\n",
    "    return lossAvg / counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "084b7a64-1062-44c9-9674-79d416f193f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(1, 256, VOCAB_SIZE)\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "9fbb2b5d-44f3-4e1c-bcec-832d688f2591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8])\n",
      "Element  tensor([[55.],\n",
      "        [ 0.],\n",
      "        [55.],\n",
      "        [38.]])\n",
      "h  torch.Size([4, 256])\n",
      "Ouput:  torch.Size([4, 86])\n",
      "Element  tensor([[57.],\n",
      "        [50.],\n",
      "        [52.],\n",
      "        [49.]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (4x256 and 1x256)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[113], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m batch \u001b[38;5;241m=\u001b[39m get_batch(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(batch[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m----> 6\u001b[0m outputs, loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#print(\"Loss: \", loss.item())\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/char-rnn/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/char-rnn/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[109], line 66\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, x, targets)\u001b[0m\n\u001b[1;32m     64\u001b[0m element \u001b[38;5;241m=\u001b[39m x[:, i]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mElement \u001b[39m\u001b[38;5;124m\"\u001b[39m, element)\n\u001b[0;32m---> 66\u001b[0m c, h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstmBlocks\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43melement\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m h \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstmBlocks[i](element)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh \u001b[39m\u001b[38;5;124m\"\u001b[39m, h\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     68\u001b[0m o \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinLays[i](h)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/char-rnn/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/char-rnn/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[109], line 31\u001b[0m, in \u001b[0;36mstmBlock.forward\u001b[0;34m(self, x, c_prev, h_prev)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, c_prev \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, h_prev \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m# If not first block:\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m h_prev \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 31\u001b[0m         f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWif\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mWhf(h_prev))\n\u001b[1;32m     32\u001b[0m         i \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mWii(x) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mWhi(h_prev))\n\u001b[1;32m     33\u001b[0m         c \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtanh(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mWic(x) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mWhc(h_prev))\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/char-rnn/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/char-rnn/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/char-rnn/lib/python3.12/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (4x256 and 1x256)"
     ]
    }
   ],
   "source": [
    "training_iterations = 3000\n",
    "for i in range(training_iterations):\n",
    "    optimizer.zero_grad()\n",
    "    batch = get_batch(\"train\")\n",
    "    print(batch[0].shape)\n",
    "    outputs, loss = model(batch[0], batch[1])\n",
    "    loss.backward()\n",
    "    #print(\"Loss: \", loss.item())\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c5cd796b-02a8-47e1-9c22-48d5d2ecf888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss:  3.005908076763153\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Loss: \", evaluate(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a220dcc5-fb27-48cc-ac7b-f7c9f5de0478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  llow you\n",
      "     h  \n",
      "Input:  dd to it\n",
      " ta   h \n",
      "Input:   turn an\n",
      "te    te\n",
      "Input:  ordered \n",
      "   th  t\n"
     ]
    }
   ],
   "source": [
    "## Let's do some testing:\n",
    "def infer(input_data):\n",
    "    with torch.no_grad():\n",
    "        outputs, _ = model(input_data)\n",
    "        predicted_indices = torch.argmax(outputs, dim=-1)\n",
    "        return predicted_indices\n",
    "        \n",
    "test_batch = get_batch(\"test\")\n",
    "result = infer(test_batch[0])\n",
    "for i in range(BATCH_SIZE):\n",
    "    print(\"Input: \", decode(test_batch[0].tolist()[i]))\n",
    "    print(decode(result.tolist()[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8573d3eb-3b90-49dc-857d-8b8ee5979243",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Truly terrible results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
